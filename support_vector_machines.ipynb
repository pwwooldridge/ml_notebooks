{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs in the linearly seperable case\n",
    "\n",
    "Suppose we are given N training vectors $\\{(x^{(i)}, y^{(i)})\\}, \\text{ where } x ∈ \\mathbb{R}^{D}, y ∈ \\{−1, 1\\}$. We want to learn a classifier:\n",
    "\n",
    "$\n",
    "h_{w,b}(x) = g(w^{T} x + b)\n",
    "$\n",
    "\n",
    "where $g(z) = 1$ if $z>= 0$ and $-1$ otherwise. \n",
    "\n",
    "### Distance between points and the hyperplane\n",
    "\n",
    "Consider a decision boundary as shown in the red dotted line in the figure below. How can we measure the distance $\\gamma^{(i)}$ between a training observation $x^{(i)}$ and our hyperplane?\n",
    "\n",
    "If we define $x_{0}$ to a vector on the hyperplane. Then the $x^{(i)} - x_{0}$ represents a vector from $x^{(i)}$ to the hyperplane. The dotted black line from $x^{(i)}$ to the hyperplane represents the vector whose distance is the shortest to the hyperplane. This dotted line forms a right-angled triangle with the hyperplane and we can label the unknown angle at the top of our triangle $\\theta$. The vector $w^{*}$ represents the unit vector perpendicular to the hyperplane. \n",
    "\n",
    "<img src=\"svm_distance_to_hyperplane.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Using trigonometry, and setting $f = x^{(i)} - x_{0}$: \n",
    "\n",
    "$\n",
    "\\cos{\\theta} = \\dfrac{\\text{adjacent}}{\\text{hypothenuse}}\\\\\n",
    "\\implies \\cos{\\theta} = \\dfrac{\\gamma^{(i)}}{\\|f\\|}\\\\\n",
    "\\implies \\|f\\|\\cos{\\theta} = \\gamma^{(i)}\\\\\n",
    "\\implies \\dfrac{\\|w^{*}\\|}{\\|w^{*}\\|}\\|f\\|\\cos{\\theta} = \\gamma^{(i)}\\\\\n",
    "\\implies fw^{*} = \\gamma^{(i)}\\\\\n",
    "\\implies \\dfrac{(x^{(i)} - x_{0})w}{\\|w\\|} = \\gamma^{(i)}\n",
    "$\n",
    "\n",
    "and then using $wx_{0} = -b$ we have: \n",
    "\n",
    "$\n",
    "\\implies \\dfrac{wx^{(i)} + b}{\\|w\\|} = \\gamma^{(i)}\n",
    "$\n",
    "\n",
    "### Functional and Geometric Margins\n",
    "\n",
    "Given a training example $(x^{(i)}, y^{(i)})$, we define the functional margin of $(w, b)$ with\n",
    "respect to the training example as:\n",
    "\n",
    "$\\hat{\\gamma}^{(i)} = y^{(i)}(w^{T} x^{(i)} + b)$\n",
    "\n",
    "The functional margin serves as a test function to determine whether a given training point is classified correctly. For a training example to be correctly classified $\\gamma^{(i)} \\geq 0$. \n",
    "\n",
    "One problem with the functional margin is that it can be affected by an arbirtrary scaling of $w$ and $b$. The functional margin gives a number but without a reference you can't tell if the point is actually far away or close to the decision plane. That brings us onto the definition of the geometric margin: \n",
    "\n",
    "$\\gamma^{(i)} = \\hat{\\gamma}^{(i)}/\\|w\\|$\n",
    "\n",
    "\n",
    "The geometric margin is telling you not only if the point is properly classified or not, but the magnitude of that distance in term of units of |w|. It is invariant to any scaling of $w$ or $b$.\n",
    "\n",
    "Given a training set \n",
    "\n",
    "$S = \\{(x^{(i)}, y^{(i)}); i=1 \\dotsc n\\}$ \n",
    "\n",
    "we define the geometric margin of $(w,b)$ with respect to $S$ to be the smallest of the geometric margins on the individual training examples:\n",
    "\n",
    "$\\gamma = \\min_{i=1 \\dotsc n} \\gamma ^ {(i)}$\n",
    "\n",
    "\n",
    "\n",
    "### The optimal margin classifier\n",
    "\n",
    "Linear SVM maximises the geometric margin of the training dataset such that all the training examples are correctly classified. This can be formulated as the following optimisation problem:\n",
    "\n",
    "$\\max_{w, b} \\gamma \\quad$ s.t. $\\quad \\dfrac{y^{(i)}(w^{T} x^{(i)} + b)}{\\|w\\|}\\geq \\gamma \\text{ for } i=1 \\dotsc n$\n",
    "\n",
    "For any solution that satisfies the above equations, any positively scaled multiple will also due to the fact that the geometric margin is invariant to scaling of $w$. \n",
    "\n",
    "Therefore, we can scale $w$ in such a way that $\\|w\\| = \\dfrac{1}{\\gamma}$. Also note that maximising $\\dfrac{1}{\\|w\\|}$ is the same as minimising $\\|w\\|$ which is the same as minimising $\\dfrac{1}{2}\\|w\\|^{2}$.\n",
    "\n",
    "Thus, we can reformulate the optimisation problem as:  \n",
    "\n",
    "$\\min_{w, b} \\dfrac{1}{2}\\|w\\|^{2} \\quad$ s.t. $\\quad y^{(i)}(w^{T} x^{(i)} + b)\\geq 1 \\text{ for } i=1 \\dotsc n$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
